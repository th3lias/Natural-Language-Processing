{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the information of your group!\n",
    "\n",
    "| <p style=\"text-align: center;\">First Name</p>  | <p style=\"text-align: center;\">Family Name</p> | Matr.-No. |\n",
    "| ---------------------------------------------- | ---------------------------------------------- | -------- |\n",
    "| <p style=\"text-align: left\">Elias</p>| <p style=\"text-align: left\">Mindelberger</p> | k12043382 |\n",
    "| <p style=\"text-align: left\">Pascal</p>| <p style=\"text-align: left\">Pilz</p> | k12111234 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center\">344.075 KV: Natural Language Processing (WS2022/23)</h2>\n",
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 3</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Document Classification with PyTorch and BERT</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Terms of Use</b><br>\n",
    "This  material is prepared for educational purposes at the Johannes Kepler University (JKU) Linz, and is exclusively provided to the registered students of the mentioned course at JKU. It is strictly forbidden to distribute the current file, the contents of the assignment, and its solution. The use or reproduction of this manuscript is only allowed for educational purposes in non-profit organizations, while in this case, the explicit prior acceptance of the author(s) is required.\n",
    "\n",
    "**Authors:** Navid Rekab-saz, Oleg Lesota<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of contents</h2>\n",
    "<ol>\n",
    "    <a href=\"#section-general-guidelines\"><li style=\"font-size:large;font-weight:bold\">General Guidelines</li></a>\n",
    "    <a href=\"#section-tensorboard\"><li style=\"font-size:large;font-weight:bold\">Bonus Task: Logging and Publishing Experiment Results (2 extra point)</li></a>\n",
    "    <a href=\"#section-taskA\"><li style=\"font-size:large;font-weight:bold\">Task A: Document Classification with PyTorch (25 points)</li></a>\n",
    "    <a href=\"#section-taskB\"><li style=\"font-size:large;font-weight:bold\">Task B: Document Classification with BERT (15 points)</li></a>\n",
    "    \n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-general-guidelines\"></a><h2 style=\"color:rgb(0,120,170)\">General Guidelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment objective\n",
    "This assignment aims to provide the necessary practices for learning the principles of deep learning programing in NLP using PyTorch. To this end, Task A provides the space for becoming fully familiar with PyTorch programming by implementing a \"simple\" document (sentence) classification model with PyTorch, and Task B extends this classifier with a BERT model. As the assignment requires working with PyTorch and Huggingface Transformers, please familiarize yourself with these libraries using any possible available teaching resources in particular the libraries' documentations. The assignment has in total **40 points**, and also offers **2 extra points** which can cover any missing point.\n",
    "\n",
    "This Notebook encompasses all aspects of the assignment, namely the descriptions of tasks as well as your solutions and reports. Feel free to add any required cell for solutions. The cells can contain code, reports, charts, tables, or any other material, required for the assignment. Feel free to provide the solutions in an interactive and visual way! \n",
    "\n",
    "Please discuss any unclear point in the assignment in the provided forum in MOODLE. It is also encouraged to provide answers to your peer's questions. However when submitting a post, keep in mind to avoid providing solutions. Please let the tutor(s) know shall you find any error or unclarity in the assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Libraries & Dataset\n",
    "\n",
    "The assignment should be implemented with recent versions of `Python`, `PyTorch` and, `transformers`. Any standard Python library can be used, so far that the library is free and can be simply installed using `pip` or `conda`. Examples of potentially useful libraries are `scikit-learn`, `numpy`, `scipy`, `gensim`, `nltk`, `spaCy`, and `AllenNLP`. Use the latest stable version of each library.\n",
    "\n",
    "To conduct the experiments, we use a subset of the `HumSet` dataset [1] (https://blog.thedeep.io/humset/). `HumSet` is created by the DEEP (https://www.thedeep.io) project â€“ an open source platform which aims to facilitate processing of textual data for international humanitarian response organizations. The platform enables the classification of text excerpts, extracted from news and reports into a set of domain specific classes. The provided dataset contains the classes (labels) referring to the humanitarian sectors like agriculture, health, and protection. The dataset contains an overall number of 17,301 data points. \n",
    "\n",
    "Download the dataset from the Moodle page of the course.\n",
    "\n",
    "the provided zip file consists of the following files:\n",
    "- `thedeep.subset.train.txt`: Train set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.validation.txt`: Validation set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.test.txt`: Test set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.label.txt`: Captions of the labels.\n",
    "- `thedeep.ToU.txt`: Terms of use of the dataset.\n",
    "\n",
    "[1] HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crises Response\n",
    "*Selim Fekih, Nicolo' Tamagnone, Benjamin Minixhofer, Ranjan Shrestha, Ximena Contla, Ewan Oglethorpe and Navid Rekabsaz.* \n",
    "In Findings of the 2022 Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP), December 2022.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Each group should submit the following two files:\n",
    "\n",
    "- One Jupyter Notebook file (`.ipynb`), containing all the code, results, visualizations, etc. **In the submitted Notebook, all the results and visualizations should already be present, and can be observed simply by loading the Notebook in a browser.** The Notebook must be self-contained, meaning that (if necessary) one can run all the cells from top to bottom without any error. Do not forget to put in your names and student numbers in the first cell of the Notebook. \n",
    "- The HTML file (`.html`) achieved from exporting the Jupyter Notebook to HTML (Download As HTML).\n",
    "\n",
    "You do not need to include the data files in the submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-tensorboard\"></a><h2 style=\"color:rgb(0,120,170)\">Bonus Task: Logging and Publishing Experiment Results (2 extra point)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all experiments of this assignment, use any experiment monitoring tool like [`TensorBoard`](https://www.tensorflow.org/tensorboard), [`wandb`](https://wandb.ai) to log and store all useful information about the training and evaluation of the models. Feel free to log any important aspect in particular the changes in evaluation results on validation, in training loss, and in learning rate.\n",
    "\n",
    "After finalizing all experiments and cleaning any unnecessary experiment, **provide the URL to the results monitoring page below**.\n",
    "\n",
    "For instance if using [`TensorBoard.dev`](https://tensorboard.dev), you can run the following command in the folder of log files: `tensorboard dev upload --name my_exp --logdir path/to/output_dir`, and take the provided URL to the TensorBoard's console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URL :** *EDIT!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparatory Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pascalpilz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pascalpilz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pascalpilz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import multiprocessing as mp\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import re as re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"../nlp2023_24_data/\"\n",
    "\n",
    "TRAIN_PATH = DATA_PATH + \"thedeep.subset.train.txt\"\n",
    "VAL_PATH   = DATA_PATH + \"thedeep.subset.validation.txt\"\n",
    "TEST_PATH  = DATA_PATH + \"thedeep.subset.test.txt\"\n",
    "\n",
    "LABEL_LEGEND_PATH = DATA_PATH + \"thedeep.labels.txt\"\n",
    "\n",
    "COL_NAMES = [\"_id\", \"data\", \"label\"]\n",
    "\n",
    "UNKNOWN = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5446</td>\n",
       "      <td>In addition to the immediate life-saving inter...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8812</td>\n",
       "      <td>There are approximately 2.6 million people cla...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16709</td>\n",
       "      <td>While aid imports have held up recently, comme...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3526</td>\n",
       "      <td>Heavy rainfalls as well as onrush of water fro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4928</td>\n",
       "      <td>Based on field reports 9 , the main production...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _id                                               data  label\n",
       "0   5446  In addition to the immediate life-saving inter...      9\n",
       "1   8812  There are approximately 2.6 million people cla...      3\n",
       "2  16709  While aid imports have held up recently, comme...      5\n",
       "3   3526  Heavy rainfalls as well as onrush of water fro...      0\n",
       "4   4928  Based on field reports 9 , the main production...      3"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COL_NAMES = [\"_id\", \"data\", \"label\"]\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH, names=COL_NAMES, sep=',')\n",
    "val_df   = pd.read_csv(VAL_PATH, names=COL_NAMES, sep=',')\n",
    "test_df  = pd.read_csv(TEST_PATH, names=COL_NAMES, sep=',')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Agriculture',\n",
       " 1: 'Cross',\n",
       " 2: 'Education',\n",
       " 3: 'Food',\n",
       " 4: 'Health',\n",
       " 5: 'Livelihood',\n",
       " 6: 'Logistic',\n",
       " 7: 'NFI',\n",
       " 8: 'Nutrition',\n",
       " 9: 'Protection',\n",
       " 10: 'Shelter',\n",
       " 11: 'WASH'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2name = {}\n",
    "\n",
    "with open(LABEL_LEGEND_PATH, \"r\") as f:\n",
    "    for line in f:\n",
    "        num_name = line.strip().split(',')\n",
    "        num, name = num_name\n",
    "        id2name[int(num)] = name\n",
    "\n",
    "id2name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskA\"></a><h2 style=\"color:rgb(0,120,170)\">Task A: Document Classification with PyTorch (25 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this task is identical to the one of Assignment 2 - Task B, namely to design a document classification model that exploits pre-trained word embeddings. It is of course allowed to use the preprocessed text, the dictionary, or any other relevant code or processings, done in the previous assignments.\n",
    "\n",
    "In this task, you implement a document classification model using PyTorch, which given a document/sentence (consisting of a set of words) predicts the corresponding class. Before getting started with coding, have a look at the <a href=\"#section-tensorboard\">optional task</a>, as you may want to already include `Tensorboard` in the code. The implementation of the classifier should cover the points below.\n",
    "\n",
    "**Preprocessing and dictionary (1 point):** Following previous assignments, load the train, validation, and test datasets, apply necessary preprocessing steps, and create a dictionary of words. \n",
    "\n",
    "**Data batching (4 points):** Using the dictionary, create batches for any given dataset (train/validation/test). Each batch is a two-dimensional matrix of *batch-size* to *max-document-length*, containing the IDs of the words in the corresponding documents. *Batch-size* and *max-document-length* are two hyper-parameters and can be set to any appropriate values (*Batch-size* must be higher than 1 and *max-document-length* at least 50 words). If a document has more than *max-document-length* words, only the first *max-document-length* words should be kept.\n",
    "\n",
    "**Word embedding lookup (2 point):** Using `torch.nn.Embedding`, create a lookup for the embeddings of all the words in the dictionary. The lookup is in fact a matrix, which maps the ID of each word to the corresponding word vector. Similar to Assignment 2, use the pre-trained vectors of a word embedding model (like word2vec or GloVe) to initialize the word embeddings of the lookup. Keep in mind that the embeddings of the words in the lookup should be matched with the correct vector in the pretrained word embedding. If the vector of a word in the lookup does not exist in the pretrained word embeddings, the corresponding vector should be initialized randomly. \n",
    "\n",
    "**Model definition (3 points):** Define the class `ClassificationAverageModel` as a PyTorch model. In the initialization procedure, the model receives the word embedding lookup, and includes it in the model as model's parameters. These embeddings parameters should be trainable, meaning that the word vectors get updated during model training. Feel free to add any other parameters to the model, which might be necessary for accomplishing the functionalities explained in the following.\n",
    "\n",
    "**Forward function (5 points):** The forward function of the model receives a batch of data, and first fetches the corresponding embeddings of the word IDs in the batch using the lookup. Similar to Assignment 2, the embedding of a document is created by calculating the *element-wise mean* of the embeddings of the document's words. Formally, given the document $d$, consisting of words $\\left[ v_1, v_2, ..., v_{|d|} \\right]$, the document representation $\\mathbf{e}_d$ is defined as:\n",
    "\n",
    "<center><div>$\\mathbf{e}_d = \\frac{1}{|d|}\\sum_{i=1}^{|d|}{\\mathbf{e}_{v_i}}$</div></center>\n",
    "\n",
    "where $\\mathbf{e}_{v}$ is the vector of the word $v$, and $|d|$ is the length of the document. An important point in the implementation of this formula is that the documents in the batch might have different lengths and therefore each document should be divided by its corresponding $|d|$. Finally, this document embedding is utilized to predict the probability of the output classes, done by applying a linear transformation from the embeddings size to the number of classes, followed by Softmax. The linear transformation also belongs to the model's parameters and will be learned in training.\n",
    "\n",
    "**Loss Function and optimization (2 point):** The loss between the predicted and the actual classes is calculated using Negative Log Likelihood or Cross Entropy. Update the model's parameters using any appropriate optimization mechanism such as Adam.\n",
    "\n",
    "**Early Stopping (2 points):** After each epoch, evaluate the model on the *validation set* using accuracy. If the evaluation result (accuracy) improves, save the model as the best performing one so far. If the results are not improving after a certain number of evaluation rounds (set as another hyper-parameter) or if training reaches a certain number of epochs, terminate the training procedure. \n",
    "\n",
    "**Test Set Evaluation (1 point):** After finishing the training, load the (already stored) best performing model, and use it for class prediction on the test set.\n",
    "\n",
    "**Reporting (1 point):** During loading and processing the collection, provide sufficient information and examples about the data and the applied processing steps. Report the results of the best performing model on the validation and test set in a table.\n",
    "\n",
    "**Overall functionality of the training procedure (4 point).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing And Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the 'word_tokenize' function from nltk for tokenization. This function incorporates multiple popular tokenizers such as TreebankWordTokenizer and PunktSentenceTokenizer into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['data'] = train_df['data'].apply(lambda x: word_tokenize(x.lower()))\n",
    "val_df['data']   = val_df['data'].apply(lambda x: word_tokenize(x.lower()))\n",
    "test_df['data']  = test_df['data'].apply(lambda x: word_tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [in, addition, to, the, immediate, life-saving...\n",
      "1        [there, are, approximately, 2.6, million, peop...\n",
      "2        [while, aid, imports, have, held, up, recently...\n",
      "3        [heavy, rainfalls, as, well, as, onrush, of, w...\n",
      "4        [based, on, field, reports, 9, ,, the, main, p...\n",
      "                               ...                        \n",
      "12105    [the, total, gap, in, the, number, of, people,...\n",
      "12106    [a, food, crisis, is, looming, in, the, countr...\n",
      "12107    [?, acute, watery, diarrhoea, (, awd, ), conti...\n",
      "12108    [as, south, india, grapples, with, drought, an...\n",
      "12109    [mirroring, trends, in, south, africa, ,, the,...\n",
      "Name: data, Length: 12110, dtype: object\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(train_df['data'])\n",
    "print(type(train_df['data'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the GoogleNews Word2Vec model for the pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../word2vec-google-news-300.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(datasets, word2vec_model):\n",
    "    word_dict = defaultdict(int)\n",
    "    idx = 0\n",
    "    word_dict['<UNK>'] = idx\n",
    "    idx += 1\n",
    "    for word in word2vec_model.key_to_index:  # Word2Vec words\n",
    "        word_dict[word] = idx\n",
    "        idx += 1\n",
    "    \n",
    "    for dataset in datasets:  # Dataset Words\n",
    "        for sentence in dataset['data']:\n",
    "            for word in sentence:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = idx\n",
    "                    idx += 1\n",
    "    return dict(word_dict)\n",
    "\n",
    "word_dict = create_dictionary([train_df, val_df, test_df], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12110.000000</td>\n",
       "      <td>12110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8666.061767</td>\n",
       "      <td>5.842527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5018.417876</td>\n",
       "      <td>3.166097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4283.500000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8684.500000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13036.750000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17300.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                _id         label\n",
       "count  12110.000000  12110.000000\n",
       "mean    8666.061767      5.842527\n",
       "std     5018.417876      3.166097\n",
       "min        1.000000      0.000000\n",
       "25%     4283.500000      3.000000\n",
       "50%     8684.500000      4.000000\n",
       "75%    13036.750000      9.000000\n",
       "max    17300.000000     11.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get summary of train df\n",
    "print(word_dict['<UNK>'])\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_word_ids(doc, word_dict, max_doc_len):\n",
    "    return [word_dict.get(word, word_dict['<UNK>']) for word in doc[:max_doc_len]]\n",
    "\n",
    "def create_batches(dataframe, word_dict, batch_size=32, max_doc_len=100):\n",
    "    doc_ids = [doc_to_word_ids(doc, word_dict, max_doc_len) for doc in dataframe['data']]                               # Convert docs to word ids\n",
    "    doc_ids_padded = np.array([np.pad(doc, (0, max(0, max_doc_len - len(doc))), 'constant') for doc in doc_ids])        # Pad docs to max_doc_len\n",
    "    \n",
    "    batches = []\n",
    "    label_batches = []\n",
    "    labels = np.array(dataframe['label'])\n",
    "    for i in range(0, len(doc_ids_padded), batch_size):\n",
    "        doc_batch = doc_ids_padded[i:i+batch_size]\n",
    "        label_batch = labels[i:i+batch_size]\n",
    "        if len(doc_batch) == batch_size:\n",
    "            batches.append(doc_batch)\n",
    "            label_batches.append(label_batch)\n",
    "\n",
    "    return np.array(batches), np.array(label_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "max_document_length = 50\n",
    "\n",
    "train_batches, train_labels = create_batches(train_df, word_dict, batch_size, max_document_length)\n",
    "val_batches, val_labels = create_batches(val_df, word_dict, batch_size, max_document_length)\n",
    "test_batches, test_labels = create_batches(test_df, word_dict, batch_size, max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1211, 10, 50),\n",
       " (1211, 10),\n",
       " (259, 10, 50),\n",
       " (259, 10),\n",
       " (259, 10, 50),\n",
       " (259, 10))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.shape, train_labels.shape, val_batches.shape, val_labels.shape, test_batches.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      2     698 3000001      12    2148 3000002   14500 3000003 1703581\n",
      "       5     453     599 3000001    1340 3000004     271      32      22\n",
      "    1551      13      12    5148       2 3000005     295      31     917\n",
      " 3000006       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0       0       0       0       0\n",
      "       0       0       0       0       0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_batches[0, 0])\n",
    "train_batches[0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 3 5 0 3 9 9 1 4 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_labels[0])\n",
    "train_labels[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "# torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = torch.tensor(train_batches, dtype=torch.long, device=device)\n",
    "val_batches = torch.tensor(val_batches, dtype=torch.long, device=device)\n",
    "test_batches = torch.tensor(test_batches, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_labels, dtype=torch.long, device=device)\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.long, device=device)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1211, 10, 50]),\n",
       " torch.Size([259, 10, 50]),\n",
       " torch.Size([259, 10, 50]),\n",
       " torch.Size([1211, 10]),\n",
       " torch.Size([259, 10]),\n",
       " torch.Size([259, 10]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.shape, val_batches.shape, test_batches.shape, train_labels.shape, val_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([      2,     698, 3000001,      12,    2148, 3000002,   14500, 3000003,\n",
      "        1703581,       5,     453,     599, 3000001,    1340, 3000004,     271,\n",
      "             32,      22,    1551,      13,      12,    5148,       2, 3000005,\n",
      "            295,      31,     917, 3000006,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(train_batches[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300 # embedding dimension of GoogleNews Word2Vec\n",
    "embedding_matrix = torch.zeros((len(word_dict), EMBEDDING_DIM), device=device, dtype=torch.float32)  # Create Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in word_dict.items():                                             # Fill embedding matrix\n",
    "    if word in model.key_to_index:\n",
    "        embedding_matrix[idx] = torch.tensor(model[word])                                     # Word2Vec\n",
    "    else:\n",
    "        embedding_matrix[idx] = torch.randn(EMBEDDING_DIM)                                    # Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('</s>', 1),\n",
       " tensor([ 1.1292e-03, -8.9645e-04,  3.1853e-04,  1.5335e-03,  1.1063e-03,\n",
       "         -1.4038e-03, -3.0518e-05, -4.1962e-04, -5.7602e-04,  1.0757e-03,\n",
       "         -1.0223e-03, -6.1798e-04, -7.5531e-04,  1.4038e-03, -1.6403e-03,\n",
       "         -6.3324e-04,  1.6327e-03, -1.0071e-03, -1.2665e-03,  6.5231e-04,\n",
       "         -4.1580e-04, -1.0757e-03,  1.5259e-03, -2.7466e-04,  1.4019e-04,\n",
       "          1.5717e-03,  1.3580e-03, -8.3160e-04, -1.4038e-03,  1.5793e-03,\n",
       "          2.5368e-04, -7.3242e-04, -1.0538e-04, -1.1673e-03,  1.5793e-03,\n",
       "          6.5613e-04, -6.5994e-04,  2.9206e-06,  1.1292e-03,  4.2725e-04,\n",
       "         -3.7003e-04, -1.1520e-03,  1.2665e-03, -3.5167e-06,  2.6512e-04,\n",
       "         -4.0245e-04,  1.4114e-04, -3.3617e-05,  7.5912e-04, -5.1880e-04,\n",
       "         -7.1049e-05,  6.0272e-04, -5.0735e-04, -1.6251e-03, -4.3678e-04,\n",
       "         -9.9182e-04, -1.2207e-03, -3.2234e-04,  6.8665e-05, -1.1673e-03,\n",
       "         -5.1117e-04,  1.4114e-03,  3.3569e-04, -4.7493e-04, -1.3733e-03,\n",
       "          3.6621e-04, -1.4420e-03, -6.0654e-04,  8.0109e-04,  1.1292e-03,\n",
       "         -8.3542e-04, -1.1597e-03,  9.1553e-04,  5.2261e-04, -3.2806e-04,\n",
       "          1.5945e-03, -1.5793e-03, -3.5667e-04,  4.9591e-04,  1.0147e-03,\n",
       "         -1.0986e-03, -1.6594e-04, -1.4210e-04, -2.6131e-04,  1.2589e-03,\n",
       "          3.8624e-05,  1.6880e-04, -1.0300e-03,  1.6098e-03,  6.2943e-04,\n",
       "          4.1771e-04, -1.3504e-03,  3.4904e-04,  1.1444e-03, -1.2054e-03,\n",
       "         -1.1826e-03,  9.4986e-04,  6.0558e-05,  1.0729e-05, -6.6757e-04,\n",
       "          1.2436e-03,  6.9046e-04,  5.5552e-05, -8.6212e-04, -1.1673e-03,\n",
       "          1.2131e-03, -8.0490e-04, -8.7738e-04,  2.2793e-04, -3.9673e-04,\n",
       "         -8.5831e-04,  2.8801e-04, -1.5869e-03,  4.8447e-04, -1.1215e-03,\n",
       "          1.9670e-06, -3.7956e-04,  7.0572e-04, -1.5869e-03,  1.6251e-03,\n",
       "          1.5564e-03, -4.3106e-04,  9.8419e-04,  9.0408e-04, -1.3962e-03,\n",
       "          1.2054e-03, -7.0190e-04,  2.7084e-04, -1.2360e-03,  6.9046e-04,\n",
       "         -8.4305e-04,  1.3428e-03, -1.4343e-03, -6.7139e-04,  1.5488e-03,\n",
       "         -1.0986e-03,  1.1902e-03, -1.4267e-03, -6.8283e-04, -7.8583e-04,\n",
       "          4.8065e-04,  4.0817e-04, -6.3705e-04,  1.4496e-04, -9.7656e-04,\n",
       "          1.4496e-03,  8.4564e-07, -1.6632e-03, -3.2806e-04,  6.2943e-04,\n",
       "         -1.4343e-03, -3.4142e-04,  1.1520e-03, -5.3024e-04, -4.7112e-04,\n",
       "         -8.5068e-04, -1.3809e-03, -1.2436e-03, -1.3275e-03,  1.0757e-03,\n",
       "          1.3199e-03, -3.0327e-04, -3.7432e-05,  1.1826e-03, -1.3580e-03,\n",
       "         -1.0452e-03,  5.6744e-05, -1.0147e-03,  4.3297e-04, -1.5717e-03,\n",
       "         -9.1076e-05,  1.0605e-03, -6.0272e-04, -1.5335e-03, -1.5335e-03,\n",
       "          5.4169e-04,  1.3351e-03,  4.1199e-04, -3.1090e-04,  1.7643e-04,\n",
       "         -1.3733e-04, -6.9809e-04, -8.6212e-04, -1.0834e-03, -2.9802e-05,\n",
       "          8.0109e-04,  6.7902e-04,  3.3569e-04, -1.3885e-03,  1.3504e-03,\n",
       "          2.3460e-04, -1.3351e-03, -8.7357e-04, -7.4387e-04,  1.0834e-03,\n",
       "          6.0558e-05, -1.2665e-03,  1.1902e-03, -6.2180e-04,  1.3597e-07,\n",
       "          1.2741e-03, -9.8419e-04, -1.5488e-03,  1.5564e-03, -1.3123e-03,\n",
       "         -7.9346e-04,  1.5335e-03,  1.2970e-03, -1.8024e-04,  9.1934e-04,\n",
       "          1.2054e-03,  7.7057e-04, -1.6556e-03,  7.7057e-04,  1.4496e-03,\n",
       "         -1.3046e-03,  6.1035e-04,  6.5994e-04,  1.2589e-03,  1.4191e-03,\n",
       "         -1.2207e-03, -1.5106e-03,  1.1292e-03,  1.3428e-03,  1.6632e-03,\n",
       "         -5.7220e-04, -5.5695e-04,  3.9864e-04, -2.7084e-04,  4.9591e-04,\n",
       "          1.6098e-03, -7.0572e-04,  6.2561e-04, -9.7656e-04, -1.8978e-04,\n",
       "          9.5367e-05, -5.1880e-04, -2.0409e-04, -8.2779e-04, -1.2302e-04,\n",
       "          7.6294e-04,  3.2234e-04, -1.2436e-03,  9.9182e-04,  1.0605e-03,\n",
       "         -1.4114e-03,  9.6798e-05, -1.5564e-03,  2.1935e-04, -5.5313e-05,\n",
       "         -9.1171e-04, -1.4877e-03,  1.3657e-03, -8.4305e-04, -4.1962e-04,\n",
       "          3.2425e-04, -1.0071e-03,  1.2589e-04, -4.5586e-04,  1.9264e-04,\n",
       "         -2.6894e-04,  1.4954e-03, -1.5869e-03,  5.9128e-04, -1.4648e-03,\n",
       "          9.6512e-04, -1.2817e-03,  1.6022e-03,  1.0910e-03, -1.3123e-03,\n",
       "          1.0910e-03, -5.1117e-04,  3.4523e-04,  1.0452e-03, -2.0695e-04,\n",
       "          9.0408e-04,  6.6757e-04,  1.1063e-03, -8.7357e-04, -3.7575e-04,\n",
       "         -2.5749e-04, -9.1553e-05,  1.4343e-03, -1.1826e-03, -8.7261e-05,\n",
       "          1.3275e-03, -1.5831e-04,  1.2894e-03, -9.8419e-04, -5.4932e-04,\n",
       "         -1.5488e-03,  1.3733e-03, -6.0797e-05, -8.2397e-04,  1.3275e-03,\n",
       "          1.1597e-03,  5.6839e-04, -1.5640e-03, -1.2302e-04, -8.6308e-05],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_dict.items())[1], embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # Create embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3027188, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.num_embeddings, embedding_layer.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(torch.tensor([word_dict['the']], dtype=torch.long).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0703,  0.0869,  0.0879,  ..., -0.0476,  0.0145, -0.0625],\n",
       "        [ 0.0454,  0.1152, -0.1758,  ...,  0.2852,  0.0771, -0.0374],\n",
       "        [ 0.3684,  0.6517,  0.7991,  ...,  0.4352,  0.8263, -1.4389],\n",
       "        ...,\n",
       "        [-0.0306,  0.8546,  0.3061,  ..., -0.4516, -2.2624,  0.6981],\n",
       "        [-0.0306,  0.8546,  0.3061,  ..., -0.4516, -2.2624,  0.6981],\n",
       "        [-0.0306,  0.8546,  0.3061,  ..., -0.4516, -2.2624,  0.6981]],\n",
       "       device='cuda:0', grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(train_batches[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition & Forward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationAverageModel(nn.Module):\n",
    "    def __init__(self, embedding_layer, num_classes):\n",
    "        super(ClassificationAverageModel, self).__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.linear = nn.Linear(embedding_layer.embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, batch, lengths):\n",
    "        embedded = self.embedding_layer(batch) # Get embeddings\n",
    "        mask = torch.arange(batch.size(1)).to(device)[None, :] < lengths[:, None] # Create mask to only average over true length\n",
    "        output = self.linear(torch.sum(embedded * mask.unsqueeze(-1), dim=1) / lengths.view(-1, 1).to(embedded.dtype)) # average embeddings\n",
    "        probabilities = F.softmax(output, dim=1) # softmax\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function And Optimization And Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationAverageModel(\n",
       "  (embedding_layer): Embedding(3027188, 300)\n",
       "  (linear): Linear(in_features=300, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ClassificationAverageModel(embedding_layer, len(id2name)).to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) # include all trainable parameters into the optimization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, criterion, train_batches, val_batches, epochs=10):\n",
    "    BEST_MODEL = None\n",
    "    EARLY_STOPPING = 3\n",
    "    BEST_VAL_LOSS = float('inf')\n",
    "    early_stopping_rounds = 1\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for i, batch in tqdm(enumerate(train_batches)):\n",
    "            inputs, labels = batch[:, :-1], batch[:, -1]\n",
    "            lengths = torch.sum(inputs != word_dict['<UNK>'], dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_batches))\n",
    "        net.eval()\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(val_batches):\n",
    "            inputs, labels = batch[:, :-1], batch[:, -1]\n",
    "            lengths = torch.sum(inputs != word_dict['<UNK>'], dim=1)\n",
    "            outputs = net(inputs, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "        val_losses.append(running_loss / len(val_batches))\n",
    "        if val_losses[-1] < BEST_VAL_LOSS:\n",
    "            BEST_VAL_LOSS = val_losses[-1]\n",
    "            BEST_MODEL = net.state_dict()\n",
    "            early_stopping_rounds = 1\n",
    "        else:\n",
    "            early_stopping_rounds += 1\n",
    "            if early_stopping_rounds == EARLY_STOPPING:\n",
    "                print(f\"Early stopping after {epoch + 1} epochs\")\n",
    "                break\n",
    "        print(f\"Epoch {epoch + 1} Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n",
    "    return net, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_with_labels = torch.cat((train_batches, train_labels.unsqueeze(-1)), dim=2)\n",
    "val_batches_with_labels = torch.cat((val_batches, val_labels.unsqueeze(-1)), dim=2)\n",
    "test_batches_with_labels = torch.cat((test_batches, test_labels.unsqueeze(-1)), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1211, 10, 50]),\n",
       " torch.Size([1211, 10, 1]),\n",
       " torch.Size([1211, 10, 51]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.size(), train_labels.unsqueeze(-1).size(), train_batches_with_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 3, 5, 0, 3, 9, 9, 1, 4, 4], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches_with_labels[0][:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1211it [01:34, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 2.1821233479076922, Val Loss: 2.064991794037543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1211it [01:34, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss: 2.039342965792271, Val Loss: 2.0393741591096384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1211it [01:34, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss: 1.9890193080823348, Val Loss: 1.9766047820161208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1211it [01:34, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss: 1.9005709711329983, Val Loss: 1.9310002055407491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1211it [01:34, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss: 1.8471022602155325, Val Loss: 1.8937479704043119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1211it [01:33, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss: 1.8101089909291288, Val Loss: 1.882191565045979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net, train_losses, val_losses = train(net, optimizer, criterion, train_batches_with_labels, val_batches_with_labels, epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might also want to load an existing model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.064991794037543,\n",
       " 2.0393741591096384,\n",
       " 1.9766047820161208,\n",
       " 1.9310002055407491,\n",
       " 1.8937479704043119,\n",
       " 1.882191565045979]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, \"bestmodel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_net = torch.load(\"bestmodel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationAverageModel(\n",
       "  (embedding_layer): Embedding(3027188, 300)\n",
       "  (linear): Linear(in_features=300, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, test_batches):\n",
    "    net.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_batches):\n",
    "            inputs, labels = batch[:, :-1], batch[:, -1]\n",
    "            lengths = torch.sum(inputs != word_dict['<UNK>'], dim=1)\n",
    "            outputs = net(inputs.to(device), lengths.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "    print(f\"Test Loss: {running_loss / len(test_batches)}, Test Accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.878210491655416, Test Accuracy: 0.7586872586872587\n"
     ]
    }
   ],
   "source": [
    "test(loaded_net, test_batches_with_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks solid! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskB\"></a><h2 style=\"color:rgb(0,120,170)\">Task B: Document Classification with BERT (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task aims to conduct the same document classification as Task A, but now by utilizing a pre-trained BERT model. Feel free to reuse any code from the previous task. The implementation of the classifier should cover the points below.\n",
    "\n",
    "**Loading BERT model (2 points):** Use the `transformers` library from `huggingface` to load a (small) pre-trained BERT model. Select a BERT model according to your available resources. The available models can be found [here](https://huggingface.co/models) and [here](https://github.com/google-research/bert).\n",
    "\n",
    "**BERT tokenization (3 points):** For training BERT models, we do not need to create a dictionary anymore, as a BERT model already contains an internal subword dictionary. Following the instruction in `transformers`'s documentation, tokenize the data using the BERT model.  \n",
    "\n",
    "**Model definition and forward function (5 points):** Define the class **`ClassificationBERTModel`** as a PyTorch model. In the initialization procedure, the model receives the loaded BERT model and stores it as the model's parameter. The parameters of the BERT model should be trainable. The forward function of the model receives a batch of data, passes this batch to BERT, and achieves the corresponding document embeddings from the output of BERT. Similar to the previous task, the document embeddings are used for classification by linearly transforming document embeddings to the vectors with the number of classes, followed by applying Softmax.\n",
    "\n",
    "**Training and overall functionality (3 points):** Train the model in a similar fashion to the previous task, namely with the proper loss function, optimization, and early stoping.\n",
    "\n",
    "**Test Set Evaluation (1 point):** After finishing the training, load the (already stored) best performing model, and use it for class prediction on the test set.\n",
    "\n",
    "**Reporting (1 point):** Report the results of the best performing model on the validation and test set in a table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gaunernst/bert-tiny-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we are using \"**tiny BERT**\", found at https://huggingface.co/gaunernst/bert-tiny-uncased.\n",
    "\n",
    "@article{turc2019,\n",
    "  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\n",
    "  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n",
    "  journal={arXiv preprint arXiv:1908.08962v2 },\n",
    "  year={2019}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize the data in the training loop. This is because the `tokenizer` provides convinient functionality for batching too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize here\n",
    "train_df['data'] = train_df['data'].apply(lambda x: tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", device=device))\n",
    "val_df['data']   = val_df['data'].apply(lambda x: tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", device=device))\n",
    "test_df['data']  = test_df['data'].apply(lambda x: tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", device=device))\n",
    "\n",
    "train_df['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(dataframe, batch_size=32):\n",
    "    batches = []\n",
    "    label_batches = []\n",
    "    labels = np.array(dataframe['label'])\n",
    "    for i in range(0, len(dataframe), batch_size):\n",
    "        doc_batch = dataframe['data'][i:i+batch_size]\n",
    "        label_batch = labels[i:i+batch_size]\n",
    "        if len(doc_batch) == batch_size:\n",
    "            batches.append(doc_batch)\n",
    "            label_batches.append(label_batch)\n",
    "    return np.array(batches), np.array(label_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches, train_labels = create_batches(train_df, batch_size)\n",
    "val_batches, val_labels = create_batches(val_df, batch_size)\n",
    "test_batches, test_labels = create_batches(test_df, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass BatchEncoding to model\n",
    "train_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationBERTModel(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, loaded_bert, num_classes):\n",
    "        \n",
    "        super(ClassificationBERTModel, self).__init__()\n",
    "\n",
    "        self.bert_model = loaded_bert\n",
    "        bert_embed_size = self.bert_model.config.hidden_size\n",
    "        self.linear     = nn.Linear(bert_embed_size, num_classes)\n",
    "\n",
    "        for param in self.bert_model.parameters():  # make the model trainable\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: BatchEncoding object from tokenizer\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs = self.bert_model(**batch)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        class_output = self.linear(embeddings[:, 0, :])\n",
    "        probabilities = F.softmax(class_output, dim=1)\n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m net \u001b[38;5;241m=\u001b[39m ClassificationBERTModel(\u001b[43mmodel\u001b[49m, \u001b[38;5;28mlen\u001b[39m(id2name))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m net, train_losses, val_losses \u001b[38;5;241m=\u001b[39m train(net, optimizer, criterion, train_batches, val_batches, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# adjusted train function for the batching\n",
    "def train(net, optimizer, criterion, train_dataframe, label_dataframe, epochs=10):\n",
    "    BEST_MODEL = None\n",
    "    EARLY_STOPPING = 3\n",
    "    BEST_VAL_LOSS = float('inf')\n",
    "    early_stopping_rounds = 1\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for i, batch in tqdm(enumerate(train_dataframe)):\n",
    "            optimizer.zero_grad()\n",
    "            # make one hot encoding\n",
    "            labels = torch.zeros((len(batch), len(id2name)), device=device)\n",
    "            labels[:, label_dataframe[i]] = 1\n",
    "            outputs = net(batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_dataframe))\n",
    "        net.eval()\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(val_dataframe):\n",
    "            outputs = net(batch)\n",
    "            loss = criterion(outputs, label_dataframe[i])\n",
    "            running_loss += loss.item()\n",
    "        val_losses.append(running_loss / len(val_dataframe))\n",
    "        if val_losses[-1] < BEST_VAL_LOSS:\n",
    "            BEST_VAL_LOSS = val_losses[-1]\n",
    "            BEST_MODEL = net.state_dict()\n",
    "            early_stopping_rounds = 1\n",
    "        else:\n",
    "            early_stopping_rounds += 1\n",
    "            if early_stopping_rounds == EARLY_STOPPING:\n",
    "                print(f\"Early stopping after {epoch + 1} epochs\")\n",
    "                break\n",
    "        print(f\"Epoch {epoch + 1} Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n",
    "    return net, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ClassificationBERTModel(model, len(id2name)).to(device)\n",
    "net, train_losses, val_losses = train(net, optimizer, criterion, train_batches, train_labels, epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_batches(df, batch_size=32, max_len=100):\n",
    "    \n",
    "    batches = []\n",
    "    data    = df['data']\n",
    "    label_batches = []\n",
    "    labels  = np.array(df['label'])\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        batches.append(batch)\n",
    "        label_batches.append(labels[i:i+batch_size])\n",
    "\n",
    "    return pd.DataFrame({'batches': batches, 'labels': label_batches})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                batches  \\\n",
      "0     0    In addition to the immediate life-saving ...   \n",
      "1     10    The Bidi Bidi settlement has sprung up a...   \n",
      "2     20    More emergency supplies including bottle...   \n",
      "3     30    Ukraine's national power company said on...   \n",
      "4     40    Local officials on Thursday cited Taliba...   \n",
      "...                                                 ...   \n",
      "1206  12060    Acute Jaundice Syndrome (AJS) â€¢ As of...   \n",
      "1207  12070    Almost half of the 12.3 million popul...   \n",
      "1208  12080    The immediate answer is cholera. The ...   \n",
      "1209  12090    Despite the urgent needs, food assist...   \n",
      "1210  12100    Minimal (IPC Phase 1) outcomes are ex...   \n",
      "\n",
      "                                labels  \n",
      "0       [9, 3, 5, 0, 3, 9, 9, 1, 4, 4]  \n",
      "1       [9, 3, 8, 9, 4, 9, 9, 3, 9, 2]  \n",
      "2     [1, 10, 3, 5, 3, 4, 3, 9, 3, 11]  \n",
      "3     [7, 10, 9, 2, 9, 9, 5, 10, 4, 4]  \n",
      "4       [9, 5, 4, 7, 2, 2, 9, 4, 9, 2]  \n",
      "...                                ...  \n",
      "1206   [4, 5, 9, 9, 9, 9, 4, 10, 9, 3]  \n",
      "1207   [4, 2, 3, 9, 4, 4, 10, 8, 7, 3]  \n",
      "1208    [4, 4, 4, 3, 4, 1, 1, 4, 2, 4]  \n",
      "1209   [3, 4, 9, 3, 3, 3, 4, 11, 4, 3]  \n",
      "1210   [3, 4, 9, 8, 4, 8, 0, 4, 11, 3]  \n",
      "\n",
      "[1211 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "max_len = 100\n",
    "\n",
    "train_batches = create_batches(train_df, batch_size, max_len)\n",
    "val_batches   = create_batches(val_df, batch_size, max_len)\n",
    "test_batches  = create_batches(test_df, batch_size, max_len)\n",
    "\n",
    "print(train_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "\n",
      "ClassificationBERTModel(\n",
      "  (bert_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=128, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "print()\n",
    "\n",
    "net = ClassificationBERTModel(model, len(id2name)).to(device)\n",
    "print(net)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) # include all trainable parameters into the optimization pipeline\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, criterion, train_batches, val_batches, tokenizer, batch_len, epochs=10):\n",
    "    BEST_MODEL = None\n",
    "    EARLY_STOPPING = 3\n",
    "    BEST_VAL_LOSS = float('inf')\n",
    "    early_stopping_rounds = 1\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for idx, batch in tqdm(train_batches.iterrows()):\n",
    "            data = batch['batches'].tolist()\n",
    "            inputs = tokenizer(data,\n",
    "                               padding=True,\n",
    "                               truncation=True,\n",
    "                               max_length=batch_len,\n",
    "                               return_tensors=\"pt\")\n",
    "            labels = torch.Tensor(batch['labels'])\n",
    "            optimizer.zero_grad()\n",
    "            output = net(inputs)\n",
    "            loss   = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_batches))\n",
    "        net.eval()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Evaulation loop\n",
    "        for idx, batch in val_batches.iterrows():\n",
    "            data = batch['batches'].tolist()\n",
    "            inputs = tokenizer(data,\n",
    "                               padding=True,\n",
    "                               truncation=True,\n",
    "                               max_length=batch_len,\n",
    "                               return_tensors=\"pt\")\n",
    "            labels = batch['labels']\n",
    "            output = net(inputs)\n",
    "            loss   = criterion(output, labels)\n",
    "            running_loss += loss.item()\n",
    "        val_losses.append(running_loss / len(val_batches))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < BEST_VAL_LOSS:\n",
    "            BEST_VAL_LOSS = val_losses[-1]\n",
    "            BEST_MODEL = net.state_dict()\n",
    "            early_stopping_rounds = 1\n",
    "        else:\n",
    "            early_stopping_rounds += 1\n",
    "            if early_stopping_rounds == EARLY_STOPPING:\n",
    "                print(f\"Early stopping after {epoch + 1} epochs\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n",
    "    return net, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net, train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m(net, optimizer, criterion, train_batches, val_batches, tokenizer, batch_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "net, train_losses, val_losses = train(net, optimizer, criterion, train_batches, val_batches, tokenizer, batch_len=50, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In addition to the immediate life-saving interventions, UNICEF is taking action to protect 200 children who have arrived at the camps in Angola without their families.', 'There are approximately 2.6 million people classified in this phase, of which 0.5 million are already in Emergency (IPC Phase 4) but do not meet the threshold of more than 20 percent of the population being in this Phase, for the areas to be classified in Phase 4. The counties classified in Crisis (IPC Phase 3) are Turkana, Marsabit, West Pokot, Samburu, Isiolo and Lamu, as well as parts of Mandera, Wajir, Garissa, Baringo, Laikipia, Kilifi and Kwale. Households in this category are marginally able to meet their minimum food needs but only by more rapidly depleting their assets and thus undermining their food consumption. In the absence of adequate cross-sectoral interventions, more areas and households in these counties are expected to fall into this phase by October 2017.', 'While aid imports have held up recently, commercial food and fuel imports remain well short of pre-blockade averages. I am particularly concerned about the recent decline of commercial food imports through the Red Sea ports. Pressure on the currency and a liquidity crisis in the Yemeni banking system make imports less viable for traders. Confidence among commercial shipping companies has eroded due to delays, including as a result of inspections undertaken by the Saudiled Coalition after these vessels have been cleared by UNVIM.', 'Heavy rainfalls as well as onrush of water from the upstream Meghalaya hills in India have led to the inundation of a vast areas of croplands of Haors and low-lying areas of the northeast. Flood started on 28th March affecting six districts (Sylhet, Moulavibazar, Sunamganj, Habiganj, Netrokona and Kishoreganj) in the north east region. Rising water overflow and breched embankment in many places and inundated vast areas of croplands. It destroyed nearly-ready-for-harvesting boro rice in about 150,000 hectors areas.', 'Based on field reports 9 , the main production challenges affecting farmers during this crop growing season included a shortage of insecticides and fungicides, high costs of irrigation (especially in March), lack of access to fertilizers due to high prices and the difficulty of securing quality crop seeds. Due to drastic decline in local production, fertilizers currently available in the country are mostly imported. The price of 50kg bags of fertilizers such as urea and phosphorous averaged SYP 9 000 (USD16) and SYP 6 000 (USD10) respectively. The cost of irrigation varied by governorate and crops. For example, the cost of irrigation per hectare of wheat ranged between USD150 and USD350 in different governorates, depending on the cost of diesel and the depth of groundwater abstraction. Field reports (March 2017) indicated pest (voles, sunn pest eurygaster integriceps and leaf hoppers) outbreaks in Idleb, Hama and Aleppo. While pest outbreaks were under control in most locations, damage from leafhoppers is likely to cause significant reduction in yields in parts of Idleb.', \"Secretary-General Antonio Guterres says in a draft report that the Saudi-led coalition was responsible for more than half the children killed and injured in Yemen's civil war last year.The report, obtained Thursday by The Associated Press, said the United Nations verified 1,340 casualties and attributed 683 - representing 51 percent - to attacks carried out by the coalition.\", 'Safe access to basic services and adequate living conditions as well as access to protection assistance, in particular specialised psychosocial support. â€¢ Safe and dignified returns though, ERW survey and clearance and assisting returnee families where return areas are secure. â€¢ Identification of persons with physical disabilities, and ensure that efficient and fast referral mechanisms to multisectoral services are implemented. â€¢ Establish efficient identification, referral and adequate response to cases of GBV and establish solid information management system. â€¢ Ensure refugees, asylum seekers and migrants enjoy safe access to basic services without discrimination or threat to physical and legal safety.', 'Rigorous and coordinated planning is necessary in order to protect the Haor areas from disasters like flash flood that is currently causing losses worth several thousand crores of taka, speakers said on Thursday.  â€œThere is a lack of coordination in the planning of Haor development and protection,â€ said eminent economist Dr Wahiduddin Mahmud at the unveiling ceremony of three reports. â€œA comprehensive plan with involvement of all ministries concerned is needed for the development of the Haor areas.â€  He said since protecting haors is a priority as these water bodies are free natural reservoirs in the country.', 'Some 300 national health professionals drawn from the Federal Ministry of Health, Amhara, Harari and Tigray regions and Dire Dawa City Adm inistration were deployed to Somali region to support the regional governmentâ€™s Emergency Health Response Plan. The surge team will work together with the Regional Health Bureau and humanitarian partners to curb the ongoing drought-induced disease outbreaks in the region, including Acute W atery diarrhoea (AWD). New cases of AWD were registered in 37 woredas of Somali region in recent weeks. The Federal Ministry of Health together with the Regional Health Bureau and humanitarian partners is leading the health response from Jijiga, the capital of Somali region. ', 'In the refugee camps of Nord and Sud Ubangi, around 800 people are need of eye treatments but there are no specialists in the area.']\n",
      "[167, 784, 534, 519, 1087, 377, 713, 616, 706, 131]\n",
      "{'input_ids': tensor([[  101,  1999,  2804,  ...,     0,     0,     0],\n",
      "        [  101,  2045,  2024,  ...,     0,     0,     0],\n",
      "        [  101,  2096,  4681,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 20001,  1998,  ...,     0,     0,     0],\n",
      "        [  101,  2070,  3998,  ...,     0,     0,     0],\n",
      "        [  101,  1999,  1996,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "batch_len = 500\n",
    "\n",
    "for i, row in train_batches.iterrows():\n",
    "    data = row['batches'].tolist()\n",
    "    inputs = tokenizer(data, padding=True, truncation=True, max_length=batch_len, return_tensors=\"pt\")\n",
    "    #print(type(data[0]))\n",
    "    #print(type(data))\n",
    "    print(data)\n",
    "    print([len(item) for item in data])\n",
    "    print(inputs)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
